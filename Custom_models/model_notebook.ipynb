{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8ED0PjI8nZZ"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCUv8ALK8o4b",
        "outputId": "7067e512-e672-47c0-f7fb-8efa5f0fa2bc"
      },
      "outputs": [],
      "source": [
        "print(tf.test.gpu_device_name())\n",
        "print(tf. __version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvxS8pTF8qWA",
        "outputId": "a58596d5-25a9-4b43-af45-1feb1fced9ec"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Mount Drive\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11mC8XHo8wew"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "path = os.path.join('/content', 'drive', 'Sign language', 'DATASET')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAqKEuMF8zk6"
      },
      "outputs": [],
      "source": [
        "# Create a test and train directory\n",
        "os.makedirs(os.path.join(path, 'train'))\n",
        "os.makedirs(os.path.join(path, 'test'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-NLZB8P83Nk"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# Creating the test and train folders\n",
        "# \"\"\"\n",
        "\n",
        "# Creating empty datafram for later splitting\n",
        "df = pd.DataFrame(columns = ['loc', 'label'])\n",
        "\n",
        "data = {'loc': '', 'label': ''}\n",
        "\n",
        "# Load all images from 'data'\n",
        "curr_path = os.path.join(path, 'DATASET')\n",
        "\n",
        "# Create the train directory\n",
        "for file in os.listdir(curr_path):   \n",
        "    print(file)\n",
        "    for item in os.listdir(os.path.join(curr_path, file)):\n",
        "        data['loc'] = os.path.join(curr_path, file, item)\n",
        "        data['label'] = file\n",
        "        df = df.append(data, ignore_index = True)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDzgChyU85KI"
      },
      "outputs": [],
      "source": [
        "# # Splitting into two\n",
        "\n",
        "train = df.sample(frac= 0.75, random_state=200) #random state is a seed value\n",
        "test = df.drop(train.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EvLikId87PQ"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df0m8U8I89CQ"
      },
      "outputs": [],
      "source": [
        "# # Saving the train dataset\n",
        "train.to_csv(os.path.join(path, 'train_split.csv'))\n",
        "test.to_csv(os.path.join(path, 'test_split.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY_NOjWk9CIF"
      },
      "outputs": [],
      "source": [
        "curr_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqWYYLQz9Dxw"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# Moving images\n",
        "# \"\"\"\n",
        "\n",
        "import cv2 as cv\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(os.path.join(path, 'train_split.csv'))\n",
        "\n",
        "for _, row in train.iterrows():\n",
        "\n",
        "    curr_path = os.path.join(path, 'train', row['label'])\n",
        "  \n",
        "    if not os.path.isdir(curr_path):\n",
        "        os.makedirs(curr_path)\n",
        "    \n",
        "    image = cv.imread(row['loc'])\n",
        "\n",
        "    # Use the cvtColor() function to grayscale the image\n",
        "    rgb_image = cv.resize(cv.cvtColor(image, cv.COLOR_BGR2RGB), (128,128))\n",
        "\n",
        "    \n",
        "    loc_img = row['loc'].rfind('/')\n",
        "    image_name = row['loc'][loc_img+1:-4] + '_02' + row['loc'][-4:] \n",
        "    img_path = os.path.join(curr_path, image_name)\n",
        "    \n",
        "    print(img_path)\n",
        "    cv.imwrite(img_path, rgb_image)\n",
        "  \n",
        "test = pd.read_csv(os.path.join(path, 'test_split.csv'))\n",
        "\n",
        "for _, row in test.iterrows():\n",
        "\n",
        "    curr_path = os.path.join(path, 'test', row['label'])\n",
        "        \n",
        "    if not os.path.isdir(curr_path):\n",
        "        os.makedirs(curr_path)\n",
        "    \n",
        "    image = cv.imread(row['loc'])\n",
        "  \n",
        "    # Use the cvtColor() function to grayscale the image\n",
        "    rgb_image = cv.resize(cv.cvtColor(image, cv.COLOR_BGR2RGB), (128,128))\n",
        "  \n",
        "    loc_img = row['loc'].rfind('/')\n",
        "    img_name = row['loc'][loc_img+1:-4] + '_02' + row['loc'][-4:] \n",
        "    img_path = os.path.join(curr_path, img_name)\n",
        "\n",
        "    print(img_path)\n",
        "    cv.imwrite(img_path, rgb_image)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHgrZeJK9GFQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model Architecture\n",
        "\"\"\"\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense , Dropout\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "sz = 128\n",
        "# Step 1 - Building the CNN\n",
        "\n",
        "# Initializing the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# First convolution layer and pooling\n",
        "classifier.add(Convolution2D(32, (3, 3), input_shape=(sz, sz, 1), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# Second convolution layer and pooling\n",
        "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
        "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
        "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flattening the layers\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Adding a fully connected layer\n",
        "classifier.add(Dense(units=128, activation='relu'))\n",
        "classifier.add(Dropout(0.40))\n",
        "classifier.add(Dense(units=96, activation='relu'))\n",
        "classifier.add(Dropout(0.40))\n",
        "classifier.add(Dense(units=64, activation='relu'))\n",
        "classifier.add(Dense(units=27, activation='softmax')) # softmax for more than 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZwTfcdd_Cgh",
        "outputId": "323ef571-4a1a-4be6-b1b6-48ff036bbde4"
      },
      "outputs": [],
      "source": [
        "# Compiling the CNN\n",
        "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy for more than 2\n",
        "\n",
        "\n",
        "# Step 2 - Preparing the train/test data and training the model\n",
        "classifier.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1JQ1I9K_EJE",
        "outputId": "4632e591-d514-404a-854c-bf475830d62d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "if tf.test.gpu_device_name(): \n",
        "\n",
        "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "\n",
        "else:\n",
        "\n",
        "   print(\"Please install GPU version of TF\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Turt-y8_GBo",
        "outputId": "b1d80c8e-8521-49f7-cc6f-204aa91f48ba"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Image loading\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "data_dir_tr = os.path.join(path, 'train')\n",
        "data_dir_ts = os.path.join(path, 'test')\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        zoom_range=0.5,\n",
        "        shear_range=0.5,\n",
        "        rotation_range=35,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        vertical_flip=True,)\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "        rescale=1./255)\n",
        "\n",
        "\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(data_dir_tr,\n",
        "                                                 target_size=(sz, sz),\n",
        "                                                 batch_size=100,\n",
        "                                                 color_mode='grayscale',\n",
        "                                                 class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(data_dir_ts,\n",
        "                                            target_size=(sz , sz),\n",
        "                                            batch_size=10,\n",
        "                                            color_mode='grayscale',\n",
        "                                            class_mode='categorical') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "f72LLAbH_Xgj",
        "outputId": "9fd80918-af96-4bbe-8e57-671554b34980"
      },
      "outputs": [],
      "source": [
        "sz   = 128\n",
        "batch_size = 100\n",
        "\n",
        "with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index='0')):\n",
        "    classifier.fit_generator(\n",
        "            training_set,\n",
        "            steps_per_epoch= 120, # No of images in training set\n",
        "            epochs=90,\n",
        "            validation_data=test_set,\n",
        "            validation_steps=400)# No of images in test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7JnqUd5H_buQ"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "model_json = classifier.to_json()\n",
        "\n",
        "curr_path = os.path.join(path, 'models')\n",
        "\n",
        "if not os.path.isdir(curr_path):\n",
        "    os.makedirs(curr_path)\n",
        "    \n",
        "\n",
        "with open(os.path.join(curr_path, \"custom_Model_json.json\"), \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "print('Model json Saved')\n",
        "classifier.save_weights(os.path.join(curr_path, 'custom_Model_weights.h5'))\n",
        "print('Weights saved')\n",
        "classifier.save(os.path.join(curr_path,'custom_Model.h5'))\n",
        "print('Model saved')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Sign_language_for_git.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
